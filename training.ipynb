{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/H-N-Chavda/BERT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldFKn-ijG1so",
        "outputId": "1c80a47c-4e96-42da-fe78-0f7214f91631"
      },
      "id": "ldFKn-ijG1so",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BERT'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 44 (delta 7), reused 43 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (44/44), 4.64 MiB | 10.54 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dd3aee0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "dd3aee0f",
        "outputId": "42d2a01b-0c65-4884-89ab-49e096867ecb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3332587245.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertForPreTraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWikiTextBERTDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/BERT/src/bert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_heads\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLMHead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNSPHead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_from_disk\n",
        "\n",
        "from BERT.config.bert_config import BertConfig\n",
        "from BERT.src.bert import BertForPreTraining\n",
        "from BERT.src.tokenizer import Tokenizer\n",
        "from dataset import WikiTextBERTDataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18aac9b8",
      "metadata": {
        "id": "18aac9b8"
      },
      "outputs": [],
      "source": [
        "dataset = load_from_disk(\"Wikitext2\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [x['text'] for x in dataset['train'] if x['text'].strip() != \"\"]\n",
        "corpus = \" \".join(texts)\n",
        "tokenizer.train(corpus)"
      ],
      "metadata": {
        "id": "q9sjhWWSHJjO"
      },
      "id": "q9sjhWWSHJjO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de30307",
      "metadata": {
        "id": "9de30307"
      },
      "outputs": [],
      "source": [
        "train_ds = WikiTextBERTDataset(\"train\", tokenizer, max_len=64, mlm_prob=0.15)\n",
        "val_ds = WikiTextBERTDataset(\"validation\", tokenizer, max_len=64, mlm_prob=0.15)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_dl = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train size: {len(train_ds)} | Val size: {len(val_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68770f85",
      "metadata": {
        "id": "68770f85"
      },
      "outputs": [],
      "source": [
        "config = BertConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    hidden_size=128,\n",
        "    num_hidden_layers=4,\n",
        "    num_attention_heads=4,\n",
        "    intermediate_size=512,\n",
        "    max_position_embeddings=64,\n",
        "    hidden_dropout_prob=0.1,\n",
        "    attention_probs_dropout_prob=0.1,\n",
        ")\n",
        "\n",
        "model = BertForPreTraining(config).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.2, total_iters=5000)\n",
        "\n",
        "print(\"Model parameters:\", sum(p.numel() for p in model.parameters()) / 1e6, \"M\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b518db3a",
      "metadata": {
        "id": "b518db3a"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, scheduler):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        out = model(**batch)\n",
        "        loss = out[\"loss\"]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            out = model(**batch)\n",
        "            total_loss += out[\"loss\"].item()\n",
        "    return total_loss / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c6f555",
      "metadata": {
        "id": "c6c6f555"
      },
      "outputs": [],
      "source": [
        "n_epochs = 5\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
        "    train_loss = train_epoch(model, train_dl, optimizer, scheduler)\n",
        "    val_loss = evaluate(model, val_dl)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), f\"checkpoints/bert_epoch{epoch+1}.pt\")\n",
        "\n",
        "print(\"Training complete ✅\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2039dc5",
      "metadata": {
        "id": "b2039dc5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"BERT Pretraining Losses\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8407e575",
      "metadata": {
        "id": "8407e575"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "sample = next(iter(val_dl))\n",
        "for k in sample:\n",
        "    sample[k] = sample[k].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model(**sample)\n",
        "\n",
        "masked_positions = (sample[\"mlm_labels\"] != -100).nonzero(as_tuple=True)\n",
        "for i, j in zip(*masked_positions):\n",
        "    pred_id = out[\"mlm_logits\"][i, j].argmax(-1).item()\n",
        "    true_id = sample[\"mlm_labels\"][i, j].item()\n",
        "    print(f\"True: {tokenizer.decode([true_id])} | Pred: {tokenizer.decode([pred_id])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7306ea97",
      "metadata": {
        "id": "7306ea97"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "torch.save(model.state_dict(), \"checkpoints/bert_final.pt\")\n",
        "print(\"✅ Model saved to checkpoints/bert_final.pt\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}